{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kenya Airways Twitter Sentiment Analysis Model using fastText\n",
    "- FastText is an NLP library developed by the Facebook AI. It's easy to use and fast to train.\n",
    "- The core of FastText relies on the Continuous Bag of Words (CBOW) model for word representation and a hierarchical classifier to speed up training.\n",
    "- fastText replaces the softmax over labels with a hierarchical softmax.\n",
    "\n",
    "## Training Data Source - Twitter US Airline Sentiment Data on Kaggle/CrowdFlower\n",
    "- https://www.kaggle.com/jagannathrk/twitter-us-airline-sentiment-analysis/data\n",
    "\n",
    "Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, csv, datetime, json\n",
    "\n",
    "import fasttext, nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import itertools\n",
    "import emoji\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smileys Dictionary Loaded...\n"
     ]
    }
   ],
   "source": [
    "# Load smileys dictionary\n",
    "smileys_dict = {}\n",
    "with open(\"data\\smileys.txt\") as f:\n",
    "    for line in f:\n",
    "       (key, val) = line.split()\n",
    "       smileys_dict[key] = val\n",
    "        \n",
    "#print(smileys_dict)\n",
    "print('Smileys Dictionary Loaded...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contractions Dictionary Loaded...\n"
     ]
    }
   ],
   "source": [
    "# Load Contractions dictionary - takes care of slang contractions as well\n",
    "# Consider contractions package ****\n",
    "\n",
    "contractions_dict = {}\n",
    "with open(\"data\\contractions.txt\") as f:\n",
    "    for line in f:\n",
    "       (key, val) = line.split(': ')\n",
    "       contractions_dict[key] = val.strip('\\n')\n",
    "        \n",
    "#print(contractions_dict)\n",
    "print('Contractions Dictionary Loaded...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning tweets\n",
    "\n",
    "Contractions/slang cleaning, Fix misspelled word, Escaping HTML, Removal of hashtags/accounts, Removal of punctuation, Emojis/Smileys etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):    \n",
    "    \n",
    "    #Escaping HTML characters\n",
    "    tweet = BeautifulSoup(tweet,'lxml').get_text()\n",
    "   \n",
    "    #Special case not handled previously.\n",
    "    tweet = tweet.replace('\\x92',\"'\")\n",
    "    \n",
    "    #Removal of hastags/account\n",
    "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \" \", tweet).split())\n",
    "    \n",
    "    #Removal of address\n",
    "    tweet = ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "    \n",
    "    #Removal of Punctuation\n",
    "    tweet = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=]\", \" \", tweet).split())\n",
    "    \n",
    "    #Lower case\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    #CONTRACTIONS source: https://en.wikipedia.org/wiki/Contraction_%28grammar%29\n",
    "    CONTRACTIONS = contractions_dict #contractions_dict()\n",
    "    tweet = tweet.replace(\"â€™\",\"'\")\n",
    "    words = tweet.split()\n",
    "    reformed = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in words]\n",
    "    tweet = \" \".join(reformed)\n",
    "    \n",
    "    # Standardizing words\n",
    "    tweet = ''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet))\n",
    "    \n",
    "    #Deal with emoticons source: https://en.wikipedia.org/wiki/List_of_emoticons\n",
    "    SMILEY = smileys_dict #smileys_dict()  \n",
    "    words = tweet.split()\n",
    "    reformed = [SMILEY[word] if word in SMILEY else word for word in words]\n",
    "    tweet = \" \".join(reformed)\n",
    "    \n",
    "    #Deal with emojis\n",
    "    tweet = emoji.demojize(tweet)\n",
    "\n",
    "    tweet = tweet.replace(\":\",\" \")\n",
    "    tweet = ' '.join(tweet.split())\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_instance(row):\n",
    "    cur_row = []\n",
    "    #Prefix the index-ed label with __label__\n",
    "    label = \"__label__\" + row[5]  \n",
    "    cur_row.append(label)\n",
    "    cur_row.extend(nltk.word_tokenize(clean_tweet(row[14].lower())))\n",
    "    return cur_row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('data\\AirlineTwitterSentiment.csv', encoding = 'unicode_escape') \n",
    "\n",
    "#train, test = train_test_split(data, test_size=0.25)\n",
    "\n",
    "#train.to_csv (r'data\\AirlineTwitterSentiment_TRAIN.csv', index = None, header=True)\n",
    "#test.to_csv (r'data\\AirlineTwitterSentiment_TEST.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocess Done...\n"
     ]
    }
   ],
   "source": [
    "def data_preprocess(input_file, output_file, keep=1):\n",
    "    i=0\n",
    "    with open(output_file, 'w', encoding='utf-8') as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "        with open(input_file, 'r', newline='', encoding='utf-8') as csvinfile: #,encoding='latin1'\n",
    "            csv_reader = csv.reader(csvinfile, delimiter=',', quotechar='\"')\n",
    "            for row in csv_reader:\n",
    "                if row[4]!=\"MIXED\" and row[5].upper() in ['POSITIVE','NEGATIVE','NEUTRAL'] and row[14]!='':\n",
    "                    row_output = transform_instance(row)\n",
    "                    csv_writer.writerow(row_output)\n",
    "                    # print(row_output)\n",
    "                #i=i+1\n",
    "                #if i%10000 ==0:\n",
    "                #    print(i)\n",
    "            \n",
    "# Preparing the training dataset        \n",
    "data_preprocess('data\\AirlineTwitterSentiment_TRAIN.csv', 'tweets.train')\n",
    "\n",
    "# Preparing the validation dataset        \n",
    "data_preprocess('data\\AirlineTwitterSentiment_TEST.csv', 'tweets.validation')\n",
    "\n",
    "print('Data Preprocess Done...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling to offset categories imbalance.\n",
    "Category imbalance problem occurs when one label appears more often than others. In such a situation, classifiers tend to be overwhelmed by the large classes and ignore the small ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upsampling done...\n"
     ]
    }
   ],
   "source": [
    "def upsampling(input_file, output_file, ratio_upsampling=1):\n",
    "    # Create a file with equal number of tweets for each label\n",
    "    #    input_file: path to file\n",
    "    #    output_file: path to the output file\n",
    "    #    ratio_upsampling: ratio of each minority classes vs majority one. \n",
    "    #    *** 1 means there will be as much of each class than there is for the majority class \n",
    "    \n",
    "    i=0\n",
    "    counts = {}\n",
    "    dict_data_by_label = {}\n",
    "\n",
    "    # GET LABEL LIST AND GET DATA PER LABEL\n",
    "    with open(input_file, 'r', newline='', encoding='utf-8') as csvinfile: \n",
    "        csv_reader = csv.reader(csvinfile, delimiter=',', quotechar='\"')\n",
    "        for row in csv_reader:\n",
    "            counts[row[0].split()[0]] = counts.get(row[0].split()[0], 0) + 1\n",
    "            if not row[0].split()[0] in dict_data_by_label:\n",
    "                dict_data_by_label[row[0].split()[0]]=[row[0]]\n",
    "            else:\n",
    "                dict_data_by_label[row[0].split()[0]].append(row[0])\n",
    "            #i=i+1\n",
    "            #if i%10000 ==0:\n",
    "            #    print(\"read\" + str(i))\n",
    "\n",
    "    # FIND MAJORITY CLASS\n",
    "    majority_class=\"\"\n",
    "    count_majority_class=0\n",
    "    for item in dict_data_by_label:\n",
    "        if len(dict_data_by_label[item])>count_majority_class:\n",
    "            majority_class= item\n",
    "            count_majority_class=len(dict_data_by_label[item])  \n",
    "    \n",
    "    # UPSAMPLE MINORITY CLASS\n",
    "    data_upsampled=[]\n",
    "    for item in dict_data_by_label:\n",
    "        data_upsampled.extend(dict_data_by_label[item])\n",
    "        if item != majority_class:\n",
    "            items_added=0\n",
    "            items_to_add = count_majority_class - len(dict_data_by_label[item])\n",
    "            while items_added<items_to_add:\n",
    "                data_upsampled.extend(dict_data_by_label[item][:max(0,min(items_to_add-items_added,len(dict_data_by_label[item])))])\n",
    "                items_added = items_added + max(0,min(items_to_add-items_added,len(dict_data_by_label[item])))\n",
    "\n",
    "    # WRITE ALL\n",
    "    #i=0\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as txtoutfile:\n",
    "        for row in data_upsampled:\n",
    "            txtoutfile.write(row+ '\\n' )\n",
    "            #i=i+1\n",
    "            #if i%10000 ==0:\n",
    "            #    print(\"writer\" + str(i))\n",
    "\n",
    "\n",
    "upsampling( 'tweets.train','uptweets.train')\n",
    "# No need to upsample for the validation set. As it does not matter what validation set contains.\n",
    "\n",
    "print('Upsampling done...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "- https://github.com/facebookresearch/fastText/tree/master/python\n",
    "\n",
    "## Baseline 80-85% - Current Model Prediction Accuracy == 79.21%\n",
    "\n",
    "Research shows that human analysts tend to agree around 80-85% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-29 23:22:24.279551 == Training START\n",
      "\t Model trained with the hyperparameter {'lr': 0.01, 'epoch': 50, 'wordNgrams': 3, 'dim': 20}\n",
      "2019-12-29 23:22:28.767462 == Training COMPLETE.\n",
      "\n",
      "Accuracy:0.9655621473014036\n",
      "Validation:0.7898907103825137\n",
      "\n",
      "Model is quantized and Saved!\n"
     ]
    }
   ],
   "source": [
    "training_data_path ='uptweets.train' \n",
    "validation_data_path ='tweets.validation'\n",
    "model_path =''\n",
    "model_name=\"model_KQ_Sentiment.ftz\"\n",
    "\n",
    "def train():\n",
    "    print(str(datetime.datetime.now())+' == Training START')\n",
    "    try:\n",
    "        hyper_params = {\"lr\": 0.01,\n",
    "                        \"epoch\": 50,\n",
    "                        \"wordNgrams\": 3,\n",
    "                        \"dim\": 20}     \n",
    "                               \n",
    "        #print( + ' START=>')\n",
    "\n",
    "        # Train the model.\n",
    "        model = fasttext.train_supervised(input=training_data_path, **hyper_params)\n",
    "        print(\"\\t Model trained with the hyperparameter {}\".format(hyper_params))\n",
    "\n",
    "        # CHECK PERFORMANCE\n",
    "        print(str(datetime.datetime.now()) + ' == Training COMPLETE.')\n",
    "        \n",
    "        model_acc_training_set = model.test(training_data_path)\n",
    "        model_acc_validation_set = model.test(validation_data_path)\n",
    "        \n",
    "        # DISPLAY ACCURACY OF TRAINED MODEL\n",
    "        text_line = \"\\nAccuracy:\" + str(model_acc_training_set[1])  + \"\\nValidation:\" + str(model_acc_validation_set[1]) + '\\n' \n",
    "        print(text_line)\n",
    "        \n",
    "        #quantize a model to reduce the memory usage\n",
    "        model.quantize(input=training_data_path, qnorm=True, retrain=True, cutoff=100000)\n",
    "        \n",
    "        \n",
    "        model.save_model(os.path.join(model_path,model_name))    \n",
    "        \n",
    "        print(\"Model is quantized and Saved!\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Exception during training: ' + str(e) )\n",
    "\n",
    "\n",
    "# Train your model.\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['__label__negative', '__label__neutral', '__label__positive']],\n",
       " array([[9.96885896e-01, 2.63305381e-03, 5.11026010e-04]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tweet = \"KQ250 is a shit plane to Seychelles . You pay an absurd amount for a ticket for lousy entertainment , food , service and comfort . @KenyaAirways . When the heck are you going to provide a better plane to paradise .\"\n",
    "\n",
    "model.predict([new_tweet],k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['__label__negative']], array([[0.9968859]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([new_tweet],k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
